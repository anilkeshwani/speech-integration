defaults: # inherit parameter values (Hydra inheritance syntax/functionality)
  - override hydra/hydra_logging: custom
  - override hydra/job_logging: custom

# Speech-specific parameters - DSUs & Modality Tokens
speech:
  n_dsus: 5000
  use_modality_tokens: True
  deduplicate: ???

# Base models directory
extended_models_dir: /mnt/scratch-artemis/anilkeshwani/models/extended/
base_model_name: Llama-3.2-1B-${speech.n_dsus}-dsus

# Experiment management
experiments_root_dir: /mnt/scratch-artemis/anilkeshwani/experiments
config_name: ${hydra:job.config_name} # name of the config file -> used to identify job type: {cpt, sft, etc.}
output_dir: ${experiments_root_dir}/${base_model_name}-${config_name}

# Tokenizer - config for setup_llama3_tokenizer -> Llama3Tokenizer
tokenizer:
  path: ${extended_models_dir}/${base_model_name}/original/tokenizer.model
  max_seq_len: ???
  prompt_template: null
  verbose: True

# Torch compile
compile: False

# Training environment
device: cuda

# Reduced precision
dtype: bf16

# Weights and Biases (W&B) logging
wandb:
  log_dir: ${output_dir}
  project: speech-integration
  entity: anilkeshwani # if null, automatically set to username by wandb based on API key
  group: ${config_name}

# Memory management
enable_activation_checkpointing: False # True reduces memory
enable_activation_offloading: False # True reduces memory

# Debug Mode
# `None` -> don't set any PyTorch global values
# "default" or 0 -> don't error or warn on nondeterministic operations and additionally enable PyTorch CuDNN benchmark
# "warn" or 1 -> warn on nondeterministic operations and disable PyTorch CuDNN benchmark
# "error" or 2 -> error on nondeterministic operations and disable PyTorch CuDNN benchmark
debug_mode: null
