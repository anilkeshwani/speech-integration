train:
  dataset:
    source: anilkeshwani/mls-speechtokenizer-rvq_0
    split: train
    sequence_type: interleaved
    interleave_kwargs:
      sampling_rate: 24000 # *model* sampling rate: sampling rate of audio input data to SpeechTokenizer model
      downsampling_ratio: 320 # downsampling ratio for SpeechTokenizer i.e. 320 samples -> 1 token
      mean_seq_len_tokens: 39.43 # mean sequence length in tokens of the MLS en trainset stratified sample (25%)
      binom_prob: 0.1 # fraction of sequence to make up subspans -> recall: binomial expectation = np
    deduplicate: ${speech.deduplicate}
    use_modality_tokens: ${speech.use_modality_tokens}
    add_eos: True
  dataloader:
    batch_size: 16
    drop_last: True
  shuffle: True
  packed: False # TODO packing performed a priori at training time and leads to CPU RAM overflow for e.g. MLS interleaved
dev:
  dataset:
    source: anilkeshwani/mls-speechtokenizer-rvq_0
    split: validation
    sequence_type: interleaved
    interleave_kwargs:
      sampling_rate: 24000 # *model* sampling rate: sampling rate of audio input data to SpeechTokenizer model
      downsampling_ratio: 320 # downsampling ratio for SpeechTokenizer i.e. 320 samples -> 1 token
      mean_seq_len_tokens: 39.43 # mean sequence length in tokens of the MLS en trainset stratified sample (25%)
      binom_prob: 0.1 # fraction of sequence to make up subspans -> recall: binomial expectation = np
    deduplicate: ${speech.deduplicate}
    use_modality_tokens: ${speech.use_modality_tokens}
    add_eos: True
  dataloader:
    batch_size: 16
    drop_last: False
  shuffle: False
  packed: False
