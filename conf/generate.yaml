defaults: # inherit parameter values (Hydra inheritance syntax/functionality)
  - common
  - data: null

# Speech-specific parameters - DSUs & Modality Tokens
speech:
  n_dsus: null # inferred from training config if null (downgraded from mandatory in common)
  use_modality_tokens: True
  deduplicate: null # inferred from training config if null (downgraded from mandatory in common)

tokenizer:
  max_seq_len: null # null implies no truncation by the tokenizer; can set to match training e.g. 768, 2048, or else

model: ???
train_config: null # training config YAML; null resolves to model/../../torchtune_config.yaml per internal convention

gen:
  output_dir: null
  output_filename: generations.jsonl
  output_config_filename: generation_config.yaml
  use_cfg_hash_subdir: False

vllm_batch_size: 8

tokenizer_decoding:
  truncate_at_eos: True
  skip_special_tokens: True

sampling_params:
  stop_token_ids: null # NOTE: stop_token_ids set in script (tokenizer stop tokens)
  n: 1 # Number of output sequences to return for the given prompt
  temperature: 0.0
  top_p: 1 # default is 1; nucleus sampling probability set to 0.95 in vLLM docs; NOTE sum_k(prob) >= p
  max_tokens: 512
  presence_penalty: 0 # default: 0
  frequency_penalty: 0 # default: 0
  repetition_penalty: 1 # default: 1
  # Aux. or currently unused parameters
  top_k: -1 # Integer that controls the number of top tokens to consider. Set to -1 to consider all tokens.

observability: False
