# Experiment management
experiments_root_dir: /mnt/scratch-artemis/anilkeshwani/experiments
output_dir: ${experiments_root_dir}/${base_model_name}
config_name: ${hydra:job.config_name} # name of the config file -> used to identify job type: {cpt, sft, etc.}
# Debug Mode
# `None` -> don't set any PyTorch global values
# "default" or 0 -> don't error or warn on nondeterministic operations and additionally enable PyTorch CuDNN benchmark
# "warn" or 1 -> warn on nondeterministic operations and disable PyTorch CuDNN benchmark
# "error" or 2 -> error on nondeterministic operations and disable PyTorch CuDNN benchmark
debug_mode: null

# Base models directory
extended_models_dir: /mnt/scratch-artemis/anilkeshwani/models/extended/
base_model_name: Llama-3.2-1B-5000-dsus

# DSUs & Modality Tokens
n_dsus: 5000
use_modality_tokens: True

# Tokenizer - config for setup_llama3_tokenizer -> Llama3Tokenizer
tokenizer:
  path: ${extended_models_dir}/${base_model_name}/original/tokenizer.model
  max_seq_len: null # NOTE max seq. length 128_000_000 returned by `llama model describe -m Llama3.2-1B`
  prompt_template: null
  verbose: True

# Data
data:
  train:
    dataset:
      source: anilkeshwani/MLS_english_train_strat_sample_aligned_hubert # HF dataset repo
      column: text
      split: train
      sequence_type: interleaved
      deduplicate: False
      use_modality_tokens: True
      add_eos: True
    dataloader:
      batch_size: 16
      drop_last: True
    shuffle: True
    packed: False # TODO packing performed a priori at training time and leads to CPU RAM overflow for e.g. MLS interleaved
    # TODO clean up / describe the following fields (arguments)
    # split_across_pack: True # set to True for CPT; set to False for SFT
    # tokenizer: # set above
    # NOTE Following arguments copied across from text_completion_dataset docstring for reference
    # filter_fn: Optional[Callable] = None
    # **load_dataset_kwargs: Dict[str, Any]
  dev:
    dataset:
      source: anilkeshwani/MLS_english_train_strat_sample_aligned_hubert # HF dataset repo
      column: text
      split: dev
      sequence_type: interleaved
      deduplicate: False
      use_modality_tokens: True
      add_eos: True
    dataloader:
      batch_size: 16 # TODO can we increase this?
      drop_last: False
    shuffle: False
    packed: False

# Optimization
optimizer:
  lr: 2e-5
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 0.01
  amsgrad: False
  fused: True
gradient_accumulation_steps: 4
clip_grad_norm: null
# Training duration in **steps** (not iterations; i.e. factoring in grad. accum.)
max_steps: 100000 # -> Set very high; typically higher than Slurm time limit (3 days == 72 hours)

# Learning rate scheduler
lr_scheduler:
  num_warmup_steps: 0
  num_cycles: 0.5

# Performance optimizations
optimizer_in_bwd: False # https://pytorch.org/tutorials/intermediate/optimizer_step_in_backward_tutorial.html
compile: False # set it to True for better memory and performance
# Memory management
enable_activation_checkpointing: False # True reduces memory
enable_activation_offloading: False # True reduces memory

# Training environment
device: cuda

# Reduced precision
dtype: bf16

# Checkpointing
checkpointer:
  checkpoint_dir: ???
  checkpoint_files: ???
  config_json: null # if null -> resolved to ${checkpointer.checkpoint_dir}  / LLAMA_3_2_CONFIG_RELPATH
  output_dir: null # if null -> resolved to ${output_dir}/${wandb_run_name}-id_{wandb_run_id}/checkpoints
  recipe_checkpoint: null
  adapter_checkpoint: null
  model_type: "llama3_2"
  safe_serialization: True

save_steps: 1000 # should be multiple of eval_steps

# Evaluation
eval_steps: 250

# Logging
wandb:
  log_dir: ${output_dir}/wandb
  project: speech-integration
  entity: null # automatically set to username based on API key
  group: null
log_interval: 1
log_peak_memory_stats: False
